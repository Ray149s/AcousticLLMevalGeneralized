{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Model SPIDEr Score Computation (Google Colab)\n",
    "\n",
    "This notebook computes SPIDEr scores (CIDEr + SPICE) for bioacoustic captioning models.\n",
    "\n",
    "## Supported Models\n",
    "- **Qwen2-Audio** - Alibaba's audio-language model\n",
    "- **NatureLM** - Earth Species Project model (auto-removes timestamps)\n",
    "- **Gemini Flash/Pro** - Google's multimodal models\n",
    "- **Any future model** conforming to the output format\n",
    "\n",
    "## Instructions\n",
    "1. Upload this notebook to Google Colab\n",
    "2. Upload your `*_results.json` files when prompted\n",
    "3. Run all cells\n",
    "4. Download the generated SPIDEr score files\n",
    "\n",
    "## Expected File Format\n",
    "Files should be named: `{model}_{prompt}_{shots}shot_results.json`\n",
    "\n",
    "Example: `qwen_baseline_3shot_results.json`, `naturelm_ornithologist_5shot_results.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Mount Google Drive (optional - for saving results)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Install Required Packages\n",
    "!pip install pycocoevalcap typing-extensions -q\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PACKAGES INSTALLED SUCCESSFULLY\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Configure Java Environment for SPICE\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(\"Configuring Java environment for SPICE...\")\n",
    "\n",
    "# Update package list and install Java 11\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq openjdk-11-jdk-headless > /dev/null\n",
    "\n",
    "# Force system to use Java 11\n",
    "!update-alternatives --set java /usr/lib/jvm/java-11-openjdk-amd64/bin/java 2>/dev/null || true\n",
    "\n",
    "# Configure Java Environment Variables for SPICE\n",
    "os.environ['_JAVA_OPTIONS'] = (\n",
    "    '-Xmx8g '\n",
    "    '--add-opens=java.base/java.lang=ALL-UNNAMED '\n",
    "    '--add-opens=java.base/java.math=ALL-UNNAMED '\n",
    "    '--add-opens=java.base/java.util=ALL-UNNAMED '\n",
    "    '--add-opens=java.base/java.util.concurrent=ALL-UNNAMED '\n",
    "    '--add-opens=java.base/java.net=ALL-UNNAMED '\n",
    "    '--add-opens=java.base/java.text=ALL-UNNAMED '\n",
    "    '--add-opens=java.base/java.lang.reflect=ALL-UNNAMED '\n",
    "    '--add-opens=java.base/java.io=ALL-UNNAMED'\n",
    ")\n",
    "\n",
    "# Verify Installation\n",
    "result = subprocess.run(['java', '-version'], capture_output=True, text=True)\n",
    "if 'openjdk version \"11' in result.stderr:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"JAVA 11 ACTIVATED SUCCESSFULLY\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"WARNING: Java 11 activation might have failed\")\n",
    "    print(result.stderr.splitlines()[0] if result.stderr else \"No version info\")\n",
    "\n",
    "# Ensure SPICE cache directory exists\n",
    "import sys\n",
    "python_version = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "spice_cache = f'/usr/local/lib/python{python_version}/dist-packages/pycocoevalcap/spice/cache'\n",
    "os.makedirs(spice_cache, exist_ok=True)\n",
    "print(f\"SPICE cache ready: {spice_cache}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Upload Evaluation Result Files\n",
    "from google.colab import files\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"UPLOAD EVALUATION RESULT FILES\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nUpload all *_results.json files from your evaluation runs.\")\n",
    "print(\"Supported models: qwen, naturelm, flash, pro, salmonn, etc.\")\n",
    "print(\"\\nExpected naming: {model}_{prompt}_{shots}shot_results.json\")\n",
    "print(\"Example: qwen_baseline_3shot_results.json\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename in uploaded.keys():\n",
    "    dest_path = f'results/{filename}'\n",
    "    if os.path.exists(filename):\n",
    "        os.rename(filename, dest_path)\n",
    "    print(f\"  Saved: {filename}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"UPLOADED {len(uploaded)} FILES\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 5: Define Core Functions\nfrom pathlib import Path\nfrom datetime import datetime\nfrom pycocoevalcap.cider.cider import Cider\nfrom pycocoevalcap.spice.spice import Spice\nfrom pycocoevalcap.bleu.bleu import Bleu\nfrom pycocoevalcap.rouge.rouge import Rouge\nimport re\n\n# Known models that need preprocessing\nMODELS_WITH_TIMESTAMPS = ['naturelm']\n\ndef parse_filename(filename: str) -> dict:\n    \"\"\"\n    Parse model, prompt, and shots from filename.\n    Handles: model_prompt_Nshot_results.json\n    \"\"\"\n    # Remove _results.json suffix\n    base = filename.replace('_results.json', '')\n    \n    # Try to extract shots (e.g., 0shot, 3shot, 5shot)\n    shot_match = re.search(r'(\\d+)shot', base)\n    if shot_match:\n        shots = int(shot_match.group(1))\n        base = base.replace(f'{shots}shot', '').rstrip('_')\n    else:\n        shots = 0\n    \n    # Split remaining into model and prompt\n    parts = base.split('_')\n    if len(parts) >= 2:\n        model = parts[0]\n        prompt = '_'.join(parts[1:])\n    else:\n        model = parts[0] if parts else 'unknown'\n        prompt = 'baseline'\n    \n    return {'model': model, 'prompt': prompt, 'shots': shots}\n\n\ndef preprocess_naturelm_text(text: str) -> str:\n    \"\"\"\n    Remove timestamp annotations from NatureLM output.\n    \n    NatureLM outputs like:\n    \"American Woodcock calling...\\n#10.00s - 20.00s#: American Woodcock\\n#20.00s - 30.00s#: ...\"\n    \n    This function extracts only the first caption before timestamps.\n    \"\"\"\n    if not text or not isinstance(text, str):\n        return text\n    \n    # Split on newline and take first non-empty line\n    lines = text.strip().split('\\n')\n    \n    # Get first line (main caption)\n    first_line = lines[0].strip()\n    \n    # Remove any leading timestamp like \"#0.00s - 10.00s#: \"\n    timestamp_pattern = r'^#[\\d.]+s\\s*-\\s*[\\d.]+s#:\\s*'\n    first_line = re.sub(timestamp_pattern, '', first_line)\n    \n    return first_line.strip()\n\n\ndef verify_preprocessing(results_dir: Path):\n    \"\"\"\n    Find a NatureLM sample with timestamps and show before/after preprocessing.\n    This helps verify the preprocessing is working correctly.\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"PREPROCESSING VERIFICATION\")\n    print(\"=\"*60)\n\n    # Look for NatureLM files\n    naturelm_files = list(results_dir.glob('naturelm_*_results.json'))\n\n    if not naturelm_files:\n        print(\"\\nNo NatureLM files found - preprocessing verification skipped.\")\n        print(\"(This is fine if you're only evaluating Qwen/Gemini models)\")\n        return\n\n    # Load first NatureLM file\n    with open(naturelm_files[0], 'r') as f:\n        data = json.load(f)\n\n    results = data.get('results', [])\n\n    # Find a sample with timestamps (contains newlines and #)\n    sample_with_timestamps = None\n    for r in results:\n        prediction = r.get('prediction', r.get('response', ''))\n        if prediction and '\\n' in prediction and '#' in prediction:\n            sample_with_timestamps = r\n            break\n\n    if not sample_with_timestamps:\n        # Fall back to any sample\n        sample_with_timestamps = results[0] if results else None\n\n    if sample_with_timestamps:\n        original = sample_with_timestamps.get('prediction', sample_with_timestamps.get('response', ''))\n        processed = preprocess_naturelm_text(original)\n\n        print(f\"\\nFile: {naturelm_files[0].name}\")\n        print(f\"Species: {sample_with_timestamps.get('species', 'Unknown')}\")\n\n        print(\"\\n\" + \"-\"*60)\n        print(\"BEFORE preprocessing:\")\n        print(\"-\"*60)\n        # Show first 500 chars to avoid overwhelming output\n        display_text = original[:500] + \"...\" if len(original) > 500 else original\n        print(repr(display_text))\n\n        print(\"\\n\" + \"-\"*60)\n        print(\"AFTER preprocessing:\")\n        print(\"-\"*60)\n        print(repr(processed))\n\n        # Show what was removed\n        if original != processed:\n            print(\"\\n\" + \"-\"*60)\n            print(\"VERIFICATION: Timestamps successfully removed\")\n            print(f\"  Original length: {len(original)} chars\")\n            print(f\"  Processed length: {len(processed)} chars\")\n            print(f\"  Removed: {len(original) - len(processed)} chars\")\n            print(\"-\"*60)\n        else:\n            print(\"\\n(No timestamps found in this sample - text unchanged)\")\n    else:\n        print(\"\\nNo samples found in NatureLM file for verification.\")\n\n\ndef compute_spider_scores(result_data: dict, model_name: str = None):\n    \"\"\"\n    Compute CIDEr, SPICE, SPIDEr, BLEU-4, and ROUGE-L scores.\n    \n    Args:\n        result_data: JSON data from evaluation result file\n        model_name: Model name for preprocessing decisions\n    \n    Returns:\n        Dictionary with all computed metrics\n    \"\"\"\n    if not result_data:\n        return None\n\n    results = result_data.get('results', [])\n    successful_results = [r for r in results if r.get('success', False)]\n\n    if not successful_results:\n        return {\n            'total': len(results),\n            'successful': 0,\n            'cider': 0.0,\n            'spice': 0.0,\n            'spider': 0.0,\n            'bleu_4': 0.0,\n            'rouge_l': 0.0\n        }\n\n    # Determine if preprocessing is needed\n    needs_preprocessing = model_name and model_name.lower() in MODELS_WITH_TIMESTAMPS\n\n    # Prepare data in pycocoevalcap format\n    gts = {}  # ground truths (references)\n    res = {}  # results (predictions)\n\n    for idx, r in enumerate(successful_results):\n        sample_id = str(idx)\n        \n        # Get reference (ground truth)\n        reference = r.get('reference', r.get('ground_truth', ''))\n        gts[sample_id] = [reference]\n        \n        # Get prediction\n        prediction = r.get('prediction', r.get('response', ''))\n        \n        # Preprocess if needed (e.g., NatureLM timestamps)\n        if needs_preprocessing:\n            prediction = preprocess_naturelm_text(prediction)\n        \n        res[sample_id] = [prediction]\n\n    # Compute metrics\n    try:\n        # CIDEr\n        cider_scorer = Cider()\n        cider_score, _ = cider_scorer.compute_score(gts, res)\n\n        # SPICE\n        try:\n            spice_scorer = Spice()\n            spice_score, _ = spice_scorer.compute_score(gts, res)\n            print(f\"    SPICE: {spice_score:.4f}\")\n        except Exception as e:\n            print(f\"    SPICE failed: {str(e)[:50]}\")\n            spice_score = 0.0\n\n        # SPIDEr = (CIDEr + SPICE) / 2\n        spider_score = (cider_score + spice_score) / 2\n\n        # BLEU-4\n        bleu_scorer = Bleu(4)\n        bleu_scores, _ = bleu_scorer.compute_score(gts, res)\n        bleu_4 = bleu_scores[3]\n\n        # ROUGE-L\n        rouge_scorer = Rouge()\n        rouge_score, _ = rouge_scorer.compute_score(gts, res)\n\n        return {\n            'total': len(results),\n            'successful': len(successful_results),\n            'cider': float(cider_score),\n            'spice': float(spice_score),\n            'spider': float(spider_score),\n            'bleu_4': float(bleu_4),\n            'rouge_l': float(rouge_score)\n        }\n    except Exception as e:\n        print(f\"    Error: {e}\")\n        return None\n\n\nprint(\"=\"*60)\nprint(\"FUNCTIONS DEFINED SUCCESSFULLY\")\nprint(\"=\"*60)\nprint(\"\\nPreprocessing enabled for: NatureLM (removes timestamps)\")\n\n# Run preprocessing verification\nverify_preprocessing(Path('results'))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Compute SPIDEr Scores for All Uploaded Files\n",
    "from pathlib import Path\n",
    "\n",
    "results_dir = Path('results')\n",
    "all_results = []\n",
    "models_found = set()\n",
    "\n",
    "# Get all JSON files\n",
    "json_files = sorted(results_dir.glob('*_results.json'))\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPUTING SPIDEr SCORES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFound {len(json_files)} result files\\n\")\n",
    "\n",
    "for idx, filepath in enumerate(json_files):\n",
    "    filename = filepath.name\n",
    "    parsed = parse_filename(filename)\n",
    "    model = parsed['model']\n",
    "    prompt = parsed['prompt']\n",
    "    shots = parsed['shots']\n",
    "    \n",
    "    config_name = f\"{model}_{prompt}_{shots}shot\"\n",
    "    models_found.add(model)\n",
    "    \n",
    "    print(f\"[{idx+1}/{len(json_files)}] {config_name}\")\n",
    "    \n",
    "    # Load result data\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            result_data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"    Failed to load: {e}\")\n",
    "        all_results.append({\n",
    "            'config': config_name,\n",
    "            'model': model,\n",
    "            'prompt': prompt,\n",
    "            'shots': shots,\n",
    "            'status': 'load_error',\n",
    "            'metrics': None\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = compute_spider_scores(result_data, model_name=model)\n",
    "    \n",
    "    if metrics:\n",
    "        print(f\"    SPIDEr: {metrics['spider']:.4f} (CIDEr: {metrics['cider']:.4f})\")\n",
    "        all_results.append({\n",
    "            'config': config_name,\n",
    "            'model': model,\n",
    "            'prompt': prompt,\n",
    "            'shots': shots,\n",
    "            'status': 'success',\n",
    "            'metrics': metrics\n",
    "        })\n",
    "    else:\n",
    "        print(f\"    Failed to compute metrics\")\n",
    "        all_results.append({\n",
    "            'config': config_name,\n",
    "            'model': model,\n",
    "            'prompt': prompt,\n",
    "            'shots': shots,\n",
    "            'status': 'error',\n",
    "            'metrics': None\n",
    "        })\n",
    "\n",
    "successful = sum(1 for r in all_results if r['status'] == 'success')\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"COMPUTATION COMPLETE: {successful}/{len(json_files)} successful\")\n",
    "print(f\"Models found: {', '.join(sorted(models_found))}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Generate Output Files\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().isoformat()\n",
    "\n",
    "# Create output for each model\n",
    "output_files = []\n",
    "\n",
    "for model in sorted(models_found):\n",
    "    model_results = [r for r in all_results if r['model'] == model]\n",
    "    successful_count = sum(1 for r in model_results if r['status'] == 'success')\n",
    "    \n",
    "    output_data = {\n",
    "        'timestamp': timestamp,\n",
    "        'model': model,\n",
    "        'total_configs': len(model_results),\n",
    "        'successful_configs': successful_count,\n",
    "        'spice_enabled': True,\n",
    "        'preprocessing_applied': model.lower() in MODELS_WITH_TIMESTAMPS,\n",
    "        'results': model_results\n",
    "    }\n",
    "    \n",
    "    output_filename = f'spider_scores_{model}.json'\n",
    "    with open(output_filename, 'w') as f:\n",
    "        json.dump(output_data, f, indent=2)\n",
    "    \n",
    "    output_files.append(output_filename)\n",
    "    print(f\"Saved: {output_filename} ({successful_count} configs)\")\n",
    "\n",
    "# Create combined output\n",
    "combined_output = {\n",
    "    'timestamp': timestamp,\n",
    "    'models': list(sorted(models_found)),\n",
    "    'total_configs': len(all_results),\n",
    "    'successful_configs': sum(1 for r in all_results if r['status'] == 'success'),\n",
    "    'spice_enabled': True,\n",
    "    'results': all_results\n",
    "}\n",
    "\n",
    "combined_filename = 'spider_scores_all_models.json'\n",
    "with open(combined_filename, 'w') as f:\n",
    "    json.dump(combined_output, f, indent=2)\n",
    "output_files.append(combined_filename)\n",
    "\n",
    "print(f\"\\nSaved combined file: {combined_filename}\")\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"GENERATED {len(output_files)} OUTPUT FILES\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Display Descriptive Statistics\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DESCRIPTIVE STATISTICS: SPIDEr SCORES BY MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create DataFrame for easier analysis\n",
    "successful_results = [r for r in all_results if r['status'] == 'success']\n",
    "\n",
    "if not successful_results:\n",
    "    print(\"\\nNo successful results to analyze.\")\n",
    "else:\n",
    "    # Build data for DataFrame\n",
    "    data = []\n",
    "    for r in successful_results:\n",
    "        data.append({\n",
    "            'Model': r['model'],\n",
    "            'Prompt': r['prompt'],\n",
    "            'Shots': r['shots'],\n",
    "            'Config': r['config'],\n",
    "            'SPIDEr': r['metrics']['spider'],\n",
    "            'CIDEr': r['metrics']['cider'],\n",
    "            'SPICE': r['metrics']['spice'],\n",
    "            'BLEU-4': r['metrics']['bleu_4'],\n",
    "            'ROUGE-L': r['metrics']['rouge_l'],\n",
    "            'Samples': r['metrics']['successful']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # ===== PER-MODEL SUMMARY =====\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"OVERALL PERFORMANCE BY MODEL\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    model_summary = df.groupby('Model').agg({\n",
    "        'SPIDEr': ['mean', 'std', 'min', 'max'],\n",
    "        'CIDEr': 'mean',\n",
    "        'SPICE': 'mean',\n",
    "        'Config': 'count'\n",
    "    }).round(4)\n",
    "    model_summary.columns = ['SPIDEr Mean', 'SPIDEr Std', 'SPIDEr Min', 'SPIDEr Max', 'CIDEr Mean', 'SPICE Mean', 'Configs']\n",
    "    print(model_summary.to_string())\n",
    "    \n",
    "    # ===== BEST CONFIG PER MODEL =====\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"BEST CONFIGURATION PER MODEL\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for model in df['Model'].unique():\n",
    "        model_df = df[df['Model'] == model]\n",
    "        best_idx = model_df['SPIDEr'].idxmax()\n",
    "        best = model_df.loc[best_idx]\n",
    "        print(f\"\\n{model.upper()}:\")\n",
    "        print(f\"  Best: {best['Prompt']} {best['Shots']}-shot\")\n",
    "        print(f\"  SPIDEr: {best['SPIDEr']:.4f} (CIDEr: {best['CIDEr']:.4f}, SPICE: {best['SPICE']:.4f})\")\n",
    "    \n",
    "    # ===== PERFORMANCE BY PROMPT TYPE =====\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"PERFORMANCE BY PROMPT TYPE (averaged across shots)\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    prompt_summary = df.groupby(['Model', 'Prompt'])['SPIDEr'].mean().unstack(fill_value=0).round(4)\n",
    "    print(prompt_summary.to_string())\n",
    "    \n",
    "    # ===== PERFORMANCE BY SHOT COUNT =====\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"PERFORMANCE BY SHOT COUNT (averaged across prompts)\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    shot_summary = df.groupby(['Model', 'Shots'])['SPIDEr'].mean().unstack(fill_value=0).round(4)\n",
    "    print(shot_summary.to_string())\n",
    "    \n",
    "    # ===== FULL RESULTS TABLE =====\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"FULL RESULTS TABLE\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    display_df = df[['Model', 'Prompt', 'Shots', 'SPIDEr', 'CIDEr', 'SPICE', 'BLEU-4', 'ROUGE-L']].copy()\n",
    "    display_df = display_df.sort_values(['Model', 'Prompt', 'Shots'])\n",
    "    print(display_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Visualize Results (Optional)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if successful_results:\n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: SPIDEr by Model and Prompt\n",
    "    ax1 = axes[0]\n",
    "    models = df['Model'].unique()\n",
    "    prompts = df['Prompt'].unique()\n",
    "    \n",
    "    x = np.arange(len(prompts))\n",
    "    width = 0.8 / len(models)\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        model_data = df[df['Model'] == model]\n",
    "        means = [model_data[model_data['Prompt'] == p]['SPIDEr'].mean() for p in prompts]\n",
    "        ax1.bar(x + i*width - 0.4 + width/2, means, width, label=model)\n",
    "    \n",
    "    ax1.set_xlabel('Prompt Type')\n",
    "    ax1.set_ylabel('SPIDEr Score')\n",
    "    ax1.set_title('SPIDEr Score by Model and Prompt')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(prompts, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: SPIDEr by Model and Shot Count\n",
    "    ax2 = axes[1]\n",
    "    shots = sorted(df['Shots'].unique())\n",
    "    \n",
    "    for model in models:\n",
    "        model_data = df[df['Model'] == model]\n",
    "        means = [model_data[model_data['Shots'] == s]['SPIDEr'].mean() for s in shots]\n",
    "        ax2.plot(shots, means, marker='o', label=model, linewidth=2, markersize=8)\n",
    "    \n",
    "    ax2.set_xlabel('Shot Count')\n",
    "    ax2.set_ylabel('SPIDEr Score')\n",
    "    ax2.set_title('SPIDEr Score by Model and Shot Count')\n",
    "    ax2.set_xticks(shots)\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('spider_scores_visualization.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nVisualization saved: spider_scores_visualization.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Download All Result Files\n",
    "from google.colab import files\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DOWNLOADING RESULT FILES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Download all output files\n",
    "for filename in output_files:\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"Downloading: {filename}\")\n",
    "        files.download(filename)\n",
    "\n",
    "# Download visualization if it exists\n",
    "if os.path.exists('spider_scores_visualization.png'):\n",
    "    print(\"Downloading: spider_scores_visualization.png\")\n",
    "    files.download('spider_scores_visualization.png')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL FILES DOWNLOADED SUCCESSFULLY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nFiles contain TRUE SPIDEr scores with:\")\n",
    "print(\"  - CIDEr (consensus-based image description evaluation)\")\n",
    "print(\"  - SPICE (semantic propositional image caption evaluation)\")\n",
    "print(\"  - SPIDEr = (CIDEr + SPICE) / 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes on Model-Specific Preprocessing\n",
    "\n",
    "### NatureLM\n",
    "NatureLM outputs include timestamp annotations for longer audio files:\n",
    "```\n",
    "American Woodcock calling with American Robin in background.\n",
    "#10.00s - 20.00s#: American Woodcock\n",
    "#20.00s - 30.00s#: American Woodcock calling...\n",
    "```\n",
    "\n",
    "This notebook **automatically extracts only the first caption** before computing SPIDEr scores, ensuring fair comparison with other models.\n",
    "\n",
    "### Adding New Models\n",
    "To add preprocessing for a new model:\n",
    "1. Add the model name (lowercase) to `MODELS_WITH_TIMESTAMPS`\n",
    "2. Create a preprocessing function similar to `preprocess_naturelm_text()`\n",
    "3. Update `compute_spider_scores()` to call the new preprocessor\n",
    "\n",
    "### Output File Format\n",
    "Each model gets its own JSON file:\n",
    "```json\n",
    "{\n",
    "  \"model\": \"qwen\",\n",
    "  \"total_configs\": 12,\n",
    "  \"results\": [\n",
    "    {\n",
    "      \"config\": \"qwen_baseline_3shot\",\n",
    "      \"metrics\": {\n",
    "        \"spider\": 0.0723,\n",
    "        \"cider\": 0.0456,\n",
    "        \"spice\": 0.0990\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  }
 ]
}