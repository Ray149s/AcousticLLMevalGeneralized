{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": "# AcousticLLMeval - Google Colab Orchestrator\n\n**Target Hardware:** Google Colab A100 (40GB VRAM)\n\n**Models Evaluated:**\n- NatureLM-audio (EarthSpeciesProject, ~10GB VRAM)\n- SALMONN (Tsinghua, ~28GB VRAM with FP16 precision)\n- Qwen2-Audio-7B (Qwen, ~14GB VRAM with BF16 precision)\n\n**Dataset:** AnimalSpeak SPIDEr Benchmark (500 samples)\n\n---\n\n## Quick Start\n\n1. **Runtime:** Change runtime type to A100 GPU\n2. **Mount Drive:** Execute Cell 1 to mount Google Drive\n3. **Install Dependencies:** Execute Cell 2 (takes ~5 minutes)\n4. **Select Models:** Modify `MODELS_TO_RUN` in Cell 4\n5. **Run Evaluation:** Execute Cell 5 (takes ~2-8 hours depending on models)\n6. **View Results:** Results saved to Google Drive at `/content/drive/MyDrive/AcousticLLMeval_Results/`\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-header"
   },
   "source": [
    "## Cell 1: Mount Google Drive & Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive for persistent results\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory in Google Drive\n",
    "DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/AcousticLLMeval_Results'\n",
    "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"✓ Google Drive mounted successfully\")\n",
    "print(f\"✓ Results will be saved to: {DRIVE_OUTPUT_DIR}\")\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"GPU Information:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    total_vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"Total VRAM: {total_vram:.2f} GB\")\n",
    "    free_vram = torch.cuda.mem_get_info()[0] / 1e9\n",
    "    print(f\"Free VRAM: {free_vram:.2f} GB\")\n",
    "    \n",
    "    if total_vram < 30:\n",
    "        print(\"\\n⚠️  WARNING: Detected GPU has <30GB VRAM. A100 recommended.\")\n",
    "        print(\"   Go to Runtime -> Change runtime type -> A100 GPU\")\n",
    "    else:\n",
    "        print(\"\\n✓ GPU has sufficient VRAM for all models\")\n",
    "else:\n",
    "    print(\"\\n❌ ERROR: No GPU detected! Enable GPU in Colab settings.\")\n",
    "    print(\"   Go to Runtime -> Change runtime type -> T4/A100 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install-header"
   },
   "source": [
    "## Cell 2: Install Dependencies & Clone Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": "%%bash\n# Update pip\npip install --upgrade pip\n\n# Install core dependencies\necho \"Installing core dependencies...\"\npip install -q torch>=2.0.0 transformers>=4.35.0 accelerate>=0.24.0\npip install -q librosa>=0.10.0 soundfile>=0.12.0\npip install -q huggingface-hub>=0.19.0 datasets>=2.14.0\npip install -q bitsandbytes>=0.41.0  # For 8-bit quantization\n\n# Install SALMONN dependencies (openai-whisper required)\necho \"Installing SALMONN dependencies...\"\npip install -q openai-whisper\n\n# Install flash-attention for faster inference (optional but recommended)\n# Note: This can take 3-5 minutes. Comment out if you want to skip.\necho \"Installing flash-attention (may take 3-5 minutes)...\"\npip install -q flash-attn --no-build-isolation || echo \"flash-attn install failed (optional, continuing...)\"\n\n# Install evaluation metrics\necho \"Installing evaluation metrics...\"\npip install -q rouge-score nltk pycocoevalcap\n\necho \"All dependencies installed successfully!\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repos"
   },
   "outputs": [],
   "source": "# Clone SALMONN repository (required for SALMONN model)\nimport os\nimport sys\n\nif not os.path.exists('/content/SALMONN'):\n    print(\"Cloning SALMONN repository...\")\n    !git clone https://github.com/bytedance/SALMONN.git /content/SALMONN\n    print(\"✓ SALMONN repository cloned\")\nelse:\n    print(\"✓ SALMONN repository already exists\")\n\n# Checkout the correct branch (salmonn, not main)\n%cd /content/SALMONN\n!git fetch --all\n!git checkout salmonn 2>/dev/null || echo \"Already on salmonn branch\"\n%cd /content\n\n# Add SALMONN to Python path\nsys.path.insert(0, '/content/SALMONN')\n\n# Clone AcousticLLMevalGeneralized repository\nif not os.path.exists('/content/AcousticLLMevalGeneralized'):\n    print(\"\\nCloning AcousticLLMevalGeneralized repository...\")\n    !git clone https://github.com/Ray149s/AcousticLLMevalGeneralized.git /content/AcousticLLMevalGeneralized\n    print(\"✓ AcousticLLMevalGeneralized cloned\")\nelse:\n    print(\"✓ AcousticLLMevalGeneralized already exists\")\n\n# Add to Python path\nsys.path.insert(0, '/content/AcousticLLMevalGeneralized')\n\nprint(\"\\n✓ All repositories ready!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auth-header"
   },
   "source": [
    "## Cell 3: Set HuggingFace Authentication\n",
    "\n",
    "Required for downloading NatureLM-audio (requires Llama-3.1 access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auth"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from getpass import getpass\n",
    "\n",
    "# Option 1: Use Colab secrets (recommended)\n",
    "# Add HF_TOKEN to Colab secrets (left sidebar, key icon)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"✓ Authenticated using Colab secrets\")\n",
    "except:\n",
    "    # Option 2: Manual entry\n",
    "    print(\"Colab secrets not found. Please enter your HuggingFace token manually.\")\n",
    "    print(\"Get your token from: https://huggingface.co/settings/tokens\")\n",
    "    HF_TOKEN = getpass(\"Enter HuggingFace token: \")\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"✓ Authenticated manually\")\n",
    "\n",
    "# Set environment variable\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "\n",
    "print(\"\\n⚠️  IMPORTANT: Make sure you have requested access to meta-llama/Llama-3.1-8B-Instruct\")\n",
    "print(\"   https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config-header"
   },
   "source": [
    "## Cell 4: Configuration & Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config"
   },
   "outputs": [],
   "source": "import json\nfrom pathlib import Path\nfrom datetime import datetime\n\n# ==============================================================================\n# USER CONFIGURATION\n# ==============================================================================\n\n# Models to evaluate (choose one or more)\n# Options: \"NatureLM\", \"SALMONN\", \"Qwen-Audio\", \"All\"\nMODELS_TO_RUN = \"All\"  # Change to [\"NatureLM\"] or [\"SALMONN\"] for single model\n\n# Dataset configuration\nBENCHMARK_PATH = \"/content/AcousticLLMevalGeneralized/animalspeak_spider_benchmark.jsonl\"\nMAX_SAMPLES = None  # None = evaluate all 500 samples, or set to integer for testing\n\n# Output configuration\nOUTPUT_DIR = DRIVE_OUTPUT_DIR\nENABLE_CHECKPOINTING = True  # Save progress after each sample for crash recovery\n\n# Few-shot configuration\nN_SHOT_EXAMPLES = 0  # 0 for zero-shot, 3 for 3-shot, etc.\n\n# Prompt template\nPROMPT_TEMPLATE = (\n    \"Generate a descriptive bioacoustic caption for the animals heard in this audio. \"\n    \"Include species identification if possible.\"\n)\n\n# ==============================================================================\n# MODEL REGISTRY (NatureLM + SALMONN + Qwen-Audio)\n# ==============================================================================\n\n# Import wrappers\nfrom naturelm_wrapper import NatureLMWrapper\nfrom salmonn_wrapper import SalmonnWrapper\nfrom qwen_wrapper import QwenAudioWrapper\n\nMODEL_REGISTRY = {\n    \"NatureLM\": {\n        \"wrapper_class\": NatureLMWrapper,\n        \"wrapper_args\": {},  # Uses defaults from wrapper (model path is hardcoded)\n        \"vram_gb\": 10.0,\n        \"estimated_time_per_sample\": 5.0,  # seconds\n    },\n    \"SALMONN\": {\n        \"wrapper_class\": SalmonnWrapper,\n        \"wrapper_args\": {},  # FP16 by default (~28GB VRAM)\n        \"vram_gb\": 28.0,\n        \"estimated_time_per_sample\": 8.0,  # seconds\n    },\n    \"Qwen-Audio\": {\n        \"wrapper_class\": QwenAudioWrapper,\n        \"wrapper_args\": {\n            \"use_4bit\": False,  # Use full BF16 precision on A100\n        },\n        \"vram_gb\": 14.0,\n        \"estimated_time_per_sample\": 6.0,  # seconds\n    },\n}\n\n# Convert \"All\" to list of all models\nif MODELS_TO_RUN == \"All\":\n    MODELS_TO_RUN = list(MODEL_REGISTRY.keys())\nelif isinstance(MODELS_TO_RUN, str):\n    MODELS_TO_RUN = [MODELS_TO_RUN]\n\n# Validate model selection\nfor model_name in MODELS_TO_RUN:\n    if model_name not in MODEL_REGISTRY:\n        raise ValueError(f\"Invalid model: {model_name}. Available: {list(MODEL_REGISTRY.keys())}\")\n\n# Print configuration\nprint(f\"{'='*80}\")\nprint(f\"EVALUATION CONFIGURATION\")\nprint(f\"{'='*80}\")\nprint(f\"Models to evaluate: {', '.join(MODELS_TO_RUN)}\")\nprint(f\"Dataset: {BENCHMARK_PATH}\")\nprint(f\"Max samples: {MAX_SAMPLES if MAX_SAMPLES else 'All (500)'}\")\nprint(f\"Few-shot examples: {N_SHOT_EXAMPLES}\")\nprint(f\"Output directory: {OUTPUT_DIR}\")\nprint(f\"Checkpointing: {'Enabled' if ENABLE_CHECKPOINTING else 'Disabled'}\")\nprint(f\"{'='*80}\")\n\n# Estimate total time\ntotal_samples = MAX_SAMPLES if MAX_SAMPLES else 500\ntotal_time_estimate = sum(\n    MODEL_REGISTRY[model][\"estimated_time_per_sample\"] * total_samples\n    for model in MODELS_TO_RUN\n)\nprint(f\"\\nEstimated total time: {total_time_estimate / 3600:.1f} hours\")\nprint(f\"Estimated completion: {datetime.now().strftime('%Y-%m-%d')} at {(datetime.now().hour + int(total_time_estimate / 3600)) % 24:02d}:00\")\nprint()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load-data-header"
   },
   "source": [
    "## Cell 5: Load Benchmark Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-data"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def load_benchmark_data(path, max_samples=None):\n",
    "    \"\"\"Load AnimalSpeak SPIDEr benchmark from JSONL.\"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            # Skip comment lines\n",
    "            if line.strip().startswith('#'):\n",
    "                continue\n",
    "            \n",
    "            # Parse JSON\n",
    "            try:\n",
    "                sample = json.loads(line)\n",
    "                samples.append(sample)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    # Limit samples if requested\n",
    "    if max_samples:\n",
    "        samples = samples[:max_samples]\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Load benchmark\n",
    "print(\"Loading benchmark dataset...\")\n",
    "benchmark_samples = load_benchmark_data(BENCHMARK_PATH, MAX_SAMPLES)\n",
    "\n",
    "print(f\"✓ Loaded {len(benchmark_samples)} samples\")\n",
    "print(f\"\\nSample preview:\")\n",
    "print(f\"{'-'*80}\")\n",
    "sample = benchmark_samples[0]\n",
    "print(f\"ID: {sample['id']}\")\n",
    "print(f\"Species: {sample['species_common']} ({sample['species_scientific']})\")\n",
    "print(f\"Caption: {sample['caption']}\")\n",
    "print(f\"Audio URL: {sample['audio_url']}\")\n",
    "print(f\"{'-'*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-audio-header"
   },
   "source": [
    "## Cell 6: Download Audio Files (Optional - for faster inference)\n",
    "\n",
    "By default, models will download audio on-the-fly. For faster inference, you can pre-download all audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-audio"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "AUDIO_CACHE_DIR = Path(\"/content/audio_cache\")\n",
    "AUDIO_CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def download_audio_file(url, cache_dir):\n",
    "    \"\"\"Download audio file to cache directory.\"\"\"\n",
    "    filename = Path(url).name\n",
    "    filepath = cache_dir / filename\n",
    "    \n",
    "    if filepath.exists():\n",
    "        return str(filepath)\n",
    "    \n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, filepath)\n",
    "        return str(filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n",
    "        return url  # Return URL if download fails\n",
    "\n",
    "# Ask user if they want to pre-download\n",
    "PREDOWNLOAD_AUDIO = True  # Set to False to skip pre-downloading\n",
    "\n",
    "if PREDOWNLOAD_AUDIO:\n",
    "    print(f\"Pre-downloading {len(benchmark_samples)} audio files...\")\n",
    "    print(\"This may take 5-10 minutes depending on network speed.\")\n",
    "    \n",
    "    for sample in tqdm(benchmark_samples, desc=\"Downloading audio\"):\n",
    "        local_path = download_audio_file(sample['audio_url'], AUDIO_CACHE_DIR)\n",
    "        sample['local_audio_path'] = local_path\n",
    "    \n",
    "    print(f\"\\n✓ All audio files downloaded to {AUDIO_CACHE_DIR}\")\n",
    "else:\n",
    "    print(\"Skipping pre-download. Audio will be downloaded on-the-fly during evaluation.\")\n",
    "    for sample in benchmark_samples:\n",
    "        sample['local_audio_path'] = sample['audio_url']  # Use URL directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation-header"
   },
   "source": [
    "## Cell 7: Main Evaluation Loop\n",
    "\n",
    "This cell runs the evaluation for all selected models sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluation"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from universal_evaluator import UniversalEvaluator\n",
    "from datetime import datetime\n",
    "\n",
    "# Store all results\n",
    "all_results = {}\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"STARTING EVALUATION\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Models: {', '.join(MODELS_TO_RUN)}\")\n",
    "print(f\"Total samples: {len(benchmark_samples)}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Iterate through models\n",
    "for model_idx, model_name in enumerate(MODELS_TO_RUN, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL {model_idx}/{len(MODELS_TO_RUN)}: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    model_config = MODEL_REGISTRY[model_name]\n",
    "    \n",
    "    # Check VRAM before loading\n",
    "    if torch.cuda.is_available():\n",
    "        free_vram = torch.cuda.mem_get_info()[0] / 1e9\n",
    "        print(f\"Free VRAM: {free_vram:.2f} GB (required: {model_config['vram_gb']} GB)\")\n",
    "        \n",
    "        if free_vram < model_config['vram_gb']:\n",
    "            print(f\"⚠️  WARNING: Insufficient VRAM. Attempting to proceed anyway...\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize model wrapper\n",
    "        print(f\"\\nInitializing {model_name}...\")\n",
    "        wrapper_class = model_config[\"wrapper_class\"]\n",
    "        wrapper_args = model_config[\"wrapper_args\"]\n",
    "        \n",
    "        model = wrapper_class(**wrapper_args)\n",
    "        \n",
    "        # Load model\n",
    "        print(f\"Loading model weights...\")\n",
    "        model.load_model()\n",
    "        print(f\"✓ {model_name} loaded successfully\")\n",
    "        \n",
    "        # Create evaluator\n",
    "        print(f\"\\nCreating evaluator...\")\n",
    "        evaluator = UniversalEvaluator(\n",
    "            model=model,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            prompt_template=PROMPT_TEMPLATE,\n",
    "            enable_checkpointing=ENABLE_CHECKPOINTING,\n",
    "        )\n",
    "        print(f\"✓ Evaluator ready\")\n",
    "        \n",
    "        # Prepare samples for evaluator\n",
    "        eval_samples = [\n",
    "            {\n",
    "                \"audio_path\": sample[\"local_audio_path\"],\n",
    "                \"reference\": sample[\"caption\"],\n",
    "            }\n",
    "            for sample in benchmark_samples\n",
    "        ]\n",
    "        \n",
    "        # Run evaluation\n",
    "        print(f\"\\nStarting evaluation of {len(eval_samples)} samples...\")\n",
    "        print(f\"Estimated time: {model_config['estimated_time_per_sample'] * len(eval_samples) / 60:.1f} minutes\")\n",
    "        print(f\"Checkpointing: {'Enabled' if ENABLE_CHECKPOINTING else 'Disabled'}\")\n",
    "        print()\n",
    "        \n",
    "        checkpoint_path = f\"{OUTPUT_DIR}/{model_name}_checkpoint.json\"\n",
    "        \n",
    "        results = evaluator.evaluate_batch(\n",
    "            samples=eval_samples,\n",
    "            checkpoint_path=checkpoint_path if ENABLE_CHECKPOINTING else None,\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        results_filename = f\"{model_name}_{timestamp}_results.json\"\n",
    "        results_path = evaluator.save_results(filename=results_filename)\n",
    "        \n",
    "        print(f\"\\n✓ {model_name} evaluation complete!\")\n",
    "        print(f\"✓ Results saved to: {results_path}\")\n",
    "        \n",
    "        # Store results\n",
    "        all_results[model_name] = {\n",
    "            \"results\": results,\n",
    "            \"results_path\": results_path,\n",
    "        }\n",
    "        \n",
    "        # Print summary\n",
    "        evaluator.print_summary()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ ERROR evaluating {model_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        all_results[model_name] = {\n",
    "            \"error\": str(e),\n",
    "            \"traceback\": traceback.format_exc(),\n",
    "        }\n",
    "    \n",
    "    finally:\n",
    "        # Clean up model from memory\n",
    "        print(f\"\\nCleaning up {model_name}...\")\n",
    "        try:\n",
    "            model.unload()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            free_vram = torch.cuda.mem_get_info()[0] / 1e9\n",
    "            print(f\"Free VRAM after cleanup: {free_vram:.2f} GB\")\n",
    "        \n",
    "        print(f\"✓ Cleanup complete\\n\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ALL EVALUATIONS COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nResults summary:\")\n",
    "for model_name, result in all_results.items():\n",
    "    if \"error\" in result:\n",
    "        print(f\"  {model_name}: FAILED - {result['error']}\")\n",
    "    else:\n",
    "        print(f\"  {model_name}: SUCCESS - {result['results_path']}\")\n",
    "\n",
    "print(f\"\\n✓ All results saved to Google Drive: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-results-header"
   },
   "source": [
    "## Cell 8: View Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "view-results"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_results(results_path):\n",
    "    \"\"\"Load results from JSON file.\"\"\"\n",
    "    with open(results_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "\n",
    "for model_name, result in all_results.items():\n",
    "    if \"error\" in result:\n",
    "        continue\n",
    "    \n",
    "    # Load full results\n",
    "    full_results = load_results(result['results_path'])\n",
    "    metadata = full_results['metadata']\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Model': model_name,\n",
    "        'Total Samples': metadata['total_results'],\n",
    "        'Avg Latency (s)': f\"{metadata['average_latency']:.2f}\",\n",
    "        'Total Cost ($)': f\"{metadata['total_cost_usd']:.4f}\",\n",
    "        'Cost per Sample ($)': f\"{metadata['total_cost_usd'] / metadata['total_results']:.4f}\",\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"EVALUATION SUMMARY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(summary_df.to_string(index=False))\n",
    "print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "# Save summary to CSV\n",
    "summary_csv_path = f\"{OUTPUT_DIR}/evaluation_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "summary_df.to_csv(summary_csv_path, index=False)\n",
    "print(f\"✓ Summary saved to: {summary_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-results-header"
   },
   "source": [
    "## Cell 9: Download Results to Local Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-results"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "# Create ZIP archive of all results\n",
    "zip_path = f\"/content/AcousticLLMeval_Results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip\"\n",
    "\n",
    "print(f\"Creating ZIP archive...\")\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Add all result files\n",
    "    for model_name, result in all_results.items():\n",
    "        if \"results_path\" in result:\n",
    "            zipf.write(result['results_path'], arcname=os.path.basename(result['results_path']))\n",
    "    \n",
    "    # Add summary CSV\n",
    "    if os.path.exists(summary_csv_path):\n",
    "        zipf.write(summary_csv_path, arcname=os.path.basename(summary_csv_path))\n",
    "\n",
    "print(f\"✓ ZIP archive created: {zip_path}\")\n",
    "print(f\"Downloading...\")\n",
    "\n",
    "# Download ZIP file\n",
    "files.download(zip_path)\n",
    "\n",
    "print(f\"\\n✓ Download complete!\")\n",
    "print(f\"Results are also saved in Google Drive: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}