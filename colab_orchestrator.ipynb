{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Bioacoustic LLM Evaluation\n",
    "\n",
    "**Evaluation Framework for Open-Weights Audio LLMs on Bioacoustic Captioning**\n",
    "\n",
    "---\n",
    "\n",
    "## Supported Models\n",
    "| Model | VRAM | Status |\n",
    "|-------|------|--------|\n",
    "| **Qwen2-Audio-7B** | ~14GB | ✅ Working |\n",
    "| **NatureLM-audio** | ~10GB | ✅ Working |\n",
    "| **SALMONN** | ~29GB | ⏳ Future Work |\n",
    "\n",
    "## Evaluation Configurations\n",
    "- **Prompt Roles**: baseline, ornithologist, skeptical, multi-taxa\n",
    "- **Shot Configs**: 0-shot, 3-shot, 5-shot\n",
    "- **Total**: 2 models × 4 prompts × 3 shots = **24 configurations**\n",
    "\n",
    "## Dataset\n",
    "- **AnimalSpeak SPIDEr Benchmark**: 500 samples with human-written captions\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Start\n",
    "1. Set runtime to **A100 GPU** (Runtime → Change runtime type)\n",
    "2. Run cells in order\n",
    "3. Results saved to Google Drive\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU & Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Check GPU\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    total_vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"VRAM: {total_vram:.1f} GB\")\n",
    "    \n",
    "    if total_vram >= 40:\n",
    "        print(\"✓ A100 detected - All models supported\")\n",
    "    elif total_vram >= 16:\n",
    "        print(\"⚠ T4/L4 detected - Qwen and NatureLM only\")\n",
    "    else:\n",
    "        print(\"❌ Insufficient VRAM - Upgrade to A100\")\n",
    "else:\n",
    "    print(\"❌ No GPU detected!\")\n",
    "    print(\"Go to: Runtime → Change runtime type → A100 GPU\")\n",
    "\n",
    "# Mount Google Drive\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MOUNTING GOOGLE DRIVE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/AcousticLLMeval_Results'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"\\n✓ Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies\n",
    "\n",
    "This installs all required packages. Takes ~3-5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%bash\necho \"Installing dependencies...\"\n\n# Upgrade pip\npip install --upgrade pip -q\n\n# Core ML packages\npip install torch torchvision torchaudio -q\npip install transformers accelerate -q\n\n# Audio processing\npip install librosa soundfile requests -q\n\n# CRITICAL: Upgrade Pillow to fix '_Ink' import error\npip install --upgrade 'Pillow>=10.0.0' -q\n\n# NatureLM dependencies\npip install peft einops omegaconf cloudpathlib -q\npip install google-cloud-storage tensorboardx wandb timm -q\npip install pydantic-settings pydub resampy -q\npip install pandas mir-eval levenshtein memoization plumbum tensorboard -q\n\necho \"\"\necho \"============================================================\"\necho \"✓ All dependencies installed!\"\necho \"\"\necho \"⚠️  IMPORTANT: You MUST restart the runtime now!\"\necho \"   Go to: Runtime → Restart runtime\"\necho \"   Then continue from Step 3 (skip Steps 1-2)\"\necho \"============================================================\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Clone Repository & Setup NatureLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\n\n# Clone evaluation repository\nif not os.path.exists('/content/AcousticLLMevalGeneralized'):\n    print(\"Cloning AcousticLLMevalGeneralized...\")\n    !git clone https://github.com/Ray149s/AcousticLLMevalGeneralized.git /content/AcousticLLMevalGeneralized\nelse:\n    print(\"✓ Repository already exists\")\n    !cd /content/AcousticLLMevalGeneralized && git pull\n\n# Clone NatureLM\nif not os.path.exists('/content/NatureLM-audio'):\n    print(\"\\nCloning NatureLM-audio...\")\n    !git clone https://github.com/earthspecies/NatureLM-audio.git /content/NatureLM-audio\nelse:\n    print(\"✓ NatureLM-audio already exists\")\n\n# CRITICAL FIX: Patch NatureLM for newer transformers (>=4.40)\n# Multiple functions moved from modeling_utils to pytorch_utils\nprint(\"\\nPatching NatureLM for transformers compatibility...\")\nfile_path = '/content/NatureLM-audio/NatureLM/models/Qformer.py'\n\nwith open(file_path, 'r') as f:\n    content = f.read()\n\n# Check if already patched\nif 'from transformers.pytorch_utils import' not in content:\n    old_import = \"from transformers.modeling_utils import (\"\n    new_import = \"\"\"from transformers.pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\nfrom transformers.modeling_utils import (\"\"\"\n    \n    content = content.replace(old_import, new_import)\n    content = content.replace(\"    apply_chunking_to_forward,\\n\", \"\")\n    content = content.replace(\"    find_pruneable_heads_and_indices,\\n\", \"\")\n    content = content.replace(\"    prune_linear_layer,\\n\", \"\")\n    \n    with open(file_path, 'w') as f:\n        f.write(content)\n    print(\"✓ NatureLM patched\")\nelse:\n    print(\"✓ NatureLM already patched\")\n\n# Install NatureLM (no-deps to avoid conflicts)\nprint(\"\\nInstalling NatureLM...\")\n!pip install --no-deps -e /content/NatureLM-audio -q\n\n# Add to Python path\nsys.path.insert(0, '/content/AcousticLLMevalGeneralized')\nsys.path.insert(0, '/content/NatureLM-audio')\n\nprint(\"\\n✓ Setup complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: HuggingFace Authentication\n",
    "\n",
    "Required for NatureLM (needs Llama-3.1 access).\n",
    "\n",
    "**Get your token:** https://huggingface.co/settings/tokens\n",
    "\n",
    "**Request access to:** https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Try Colab secrets first\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    print(\"✓ Using HF_TOKEN from Colab secrets\")\n",
    "except:\n",
    "    # Manual entry\n",
    "    from getpass import getpass\n",
    "    print(\"Enter your HuggingFace token:\")\n",
    "    HF_TOKEN = getpass()\n",
    "\n",
    "# Login\n",
    "login(token=HF_TOKEN, add_to_git_credential=False)\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "\n",
    "print(\"✓ Authenticated with HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify Model Imports\n",
    "\n",
    "Test that both models can be imported before running evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing model imports...\\n\")\n",
    "\n",
    "# Test Qwen2Audio\n",
    "try:\n",
    "    from transformers import Qwen2AudioForConditionalGeneration, Qwen2AudioProcessor\n",
    "    print(\"✓ Qwen2Audio imports OK\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Qwen2Audio import failed: {e}\")\n",
    "\n",
    "# Test NatureLM\n",
    "try:\n",
    "    from NatureLM.models import NatureLM\n",
    "    from NatureLM.infer import Pipeline\n",
    "    print(\"✓ NatureLM imports OK\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ NatureLM import failed: {e}\")\n",
    "\n",
    "# Test prompt config\n",
    "try:\n",
    "    from prompt_config import build_prompt, PROMPT_VERSIONS, SHOT_CONFIGS\n",
    "    print(\"✓ Prompt config imports OK\")\n",
    "    print(f\"  Prompt versions: {PROMPT_VERSIONS}\")\n",
    "    print(f\"  Shot configs: {SHOT_CONFIGS}\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Prompt config import failed: {e}\")\n",
    "\n",
    "print(\"\\n✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configuration\n",
    "\n",
    "Configure which models, prompts, and shots to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATION CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Models to evaluate\n",
    "MODELS = ['qwen', 'naturelm']  # Options: 'qwen', 'naturelm'\n",
    "\n",
    "# Prompt versions (from Gemini evaluation)\n",
    "PROMPT_VERSIONS = ['baseline', 'ornithologist', 'skeptical', 'multi-taxa']\n",
    "\n",
    "# Shot configurations\n",
    "SHOT_CONFIGS = [0, 3, 5]\n",
    "\n",
    "# Dataset\n",
    "BENCHMARK_PATH = '/content/AcousticLLMevalGeneralized/animalspeak_spider_benchmark.jsonl'\n",
    "\n",
    "# Samples (None = all 500, or set integer for testing)\n",
    "MAX_SAMPLES = None  # Set to 10 for quick test\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "# Calculate total configurations\n",
    "total_configs = len(MODELS) * len(PROMPT_VERSIONS) * len(SHOT_CONFIGS)\n",
    "samples = MAX_SAMPLES if MAX_SAMPLES else 500\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Models: {MODELS}\")\n",
    "print(f\"Prompts: {PROMPT_VERSIONS}\")\n",
    "print(f\"Shots: {SHOT_CONFIGS}\")\n",
    "print(f\"Samples: {samples}\")\n",
    "print(f\"Total configs: {total_configs}\")\n",
    "print(f\"Total evaluations: {total_configs * samples:,}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Run Evaluation\n",
    "\n",
    "This runs the full evaluation across all configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/AcousticLLMevalGeneralized')\n",
    "\n",
    "from run_full_evaluation import run_full_evaluation\n",
    "\n",
    "# Run evaluation\n",
    "results = run_full_evaluation(\n",
    "    models=MODELS,\n",
    "    prompt_versions=PROMPT_VERSIONS,\n",
    "    shot_configs=SHOT_CONFIGS,\n",
    "    jsonl_path=BENCHMARK_PATH,\n",
    "    max_samples=MAX_SAMPLES,\n",
    "    output_dir=OUTPUT_DIR,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Results saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: View Sample Results\n",
    "\n",
    "Preview some example predictions from both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Find result files\n",
    "result_files = list(Path(OUTPUT_DIR).glob('*_results.json'))\n",
    "\n",
    "print(f\"Found {len(result_files)} result files\\n\")\n",
    "\n",
    "# Show sample from each model\n",
    "for model in MODELS:\n",
    "    model_files = [f for f in result_files if model in f.name]\n",
    "    if model_files:\n",
    "        # Load first config (baseline 0-shot)\n",
    "        with open(model_files[0]) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(f\"MODEL: {model.upper()}\")\n",
    "        print(f\"Config: {data.get('config_name', 'unknown')}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Show first 3 samples\n",
    "        for i, result in enumerate(data.get('results', [])[:3]):\n",
    "            print(f\"\\nSample {i+1}: {result.get('species', 'Unknown')}\")\n",
    "            print(f\"Reference: {result.get('reference', 'N/A')}\")\n",
    "            print(f\"Prediction: {result.get('prediction', 'N/A')[:100]}...\")\n",
    "            print(f\"Latency: {result.get('latency', 0):.2f}s\")\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Generate Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Collect all results\n",
    "summary_data = []\n",
    "\n",
    "for result_file in Path(OUTPUT_DIR).glob('*_results.json'):\n",
    "    with open(result_file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    model = data.get('model', result_file.stem.split('_')[0])\n",
    "    prompt = data.get('prompt_version', 'unknown')\n",
    "    shots = data.get('n_shots', 0)\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Model': model,\n",
    "        'Prompt': prompt,\n",
    "        'Shots': shots,\n",
    "        'Samples': data.get('samples_tested', 0),\n",
    "        'Success': data.get('successful', 0),\n",
    "        'Avg Latency': f\"{data.get('avg_latency', 0):.2f}s\",\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(summary_data)\n",
    "df = df.sort_values(['Model', 'Prompt', 'Shots'])\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_path = f\"{OUTPUT_DIR}/evaluation_summary.csv\"\n",
    "df.to_csv(summary_path, index=False)\n",
    "print(f\"\\n✓ Summary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Download Results\n",
    "\n",
    "Download all results as a ZIP file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from google.colab import files\n",
    "from datetime import datetime\n",
    "\n",
    "# Create ZIP\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "zip_path = f'/content/evaluation_results_{timestamp}.zip'\n",
    "\n",
    "print(\"Creating ZIP archive...\")\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for result_file in Path(OUTPUT_DIR).glob('*.json'):\n",
    "        zipf.write(result_file, result_file.name)\n",
    "    for csv_file in Path(OUTPUT_DIR).glob('*.csv'):\n",
    "        zipf.write(csv_file, csv_file.name)\n",
    "\n",
    "print(f\"✓ ZIP created: {zip_path}\")\n",
    "print(\"\\nDownloading...\")\n",
    "files.download(zip_path)\n",
    "\n",
    "print(\"\\n✓ Download complete!\")\n",
    "print(f\"Results also saved in Google Drive: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Example Prompt (3-shot baseline)\n",
    "\n",
    "This shows what prompt is sent to the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_config import build_prompt, FewShotExample\n",
    "\n",
    "# Example few-shot examples\n",
    "examples = [\n",
    "    FewShotExample(\n",
    "        caption='a chorus of squirrel treefrogs with southern leopard frogs.',\n",
    "        environment='iNaturalist'\n",
    "    ),\n",
    "    FewShotExample(\n",
    "        caption='faint song of an alder flycatcher.',\n",
    "        environment='iNaturalist'\n",
    "    ),\n",
    "    FewShotExample(\n",
    "        caption='an american woodcock calls with a series of peents at dusk.',\n",
    "        environment='iNaturalist'\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Build prompt\n",
    "prompt = build_prompt(prompt_version='baseline', examples=examples)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXAMPLE PROMPT (baseline, 3-shot)\")\n",
    "print(\"=\" * 60)\n",
    "print(prompt)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Appendix: Troubleshooting\n\n### Common Issues\n\n| Issue | Solution |\n|-------|----------|\n| `cannot import name '_Ink' from 'PIL._typing'` | Run: `pip install --upgrade 'Pillow>=10.0.0'` then **restart runtime** |\n| `Qwen2AudioForConditionalGeneration` not found | Same as above - Pillow version issue |\n| `No module named 'peft'` | Run: `pip install peft` |\n| `No module named 'NatureLM'` | Run: `pip install --no-deps -e /content/NatureLM-audio` |\n| `No module named 'run_full_evaluation'` | Run: `!cd /content/AcousticLLMevalGeneralized && git pull` |\n| Out of memory | Use `MAX_SAMPLES = 10` for testing |\n| HuggingFace auth failed | Check token and Llama-3.1 access |\n\n### After Step 2: MUST Restart Runtime\nThe Pillow upgrade requires a runtime restart to take effect:\n1. Run Step 2 (Install Dependencies)\n2. **Runtime → Restart runtime**\n3. Continue from Step 3 (skip Steps 1-2)\n\n### Quick Fix for Pillow Error\n```python\n!pip install --upgrade 'Pillow>=10.0.0'\n# Then: Runtime → Restart runtime\n```"
  }
 ]
}